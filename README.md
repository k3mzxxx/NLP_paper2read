# NLP_paper2read
此仓库用于记录本人学习NLP的论文阅读顺序，如果能帮到一些小伙伴的话，It's my pleasure.

## survey
Advances in natural language processing：对整个自然语言处理领域发展的综述  
Pre-trained Models for Natural Language Processing: A Survey 预训练模型综述  
Multimodal Foundation Models: From Specialists to General-Purpose Assistants：多模态大模型  

## baseline paper
1. ICLR2013，Word2Vec：词向量抗鼎之作，分为两篇，第一篇介绍CBOW和Skip-gram模型，第二篇介绍层次化softmax和负采样方法。
   [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)  
   [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)
   
3. EMNLP2014，Glove：词向量领域  
   [GloVe: Global Vectors for Word Representation](https://scholar.google.com/scholar?q=GloVe:+Global+Vectors+for+Word+Representation&hl=zh-CN&as_sdt=0&as_vis=1&oi=scholart)
   
5. EMNLP2015, CharEmbedding：字符嵌入  
   [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)
   
7. EMNLP2014，TextCNN：CNN 用于文本分类  
   [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)
   
9. NIPS2015，CharTextCNN：字符级别的文本分类  
   [Character-level Convolutional Networks for Text Classification](https://arxiv.org/abs/1509.01626)
   
11. EACL2017，FastText：细粒度的文本分类  
    [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759)  
    
13. NLPS2014，DeepNMT：使用LSTM解决机器翻译问题  
    [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
    
15. ICLR2015，BahdanauNMT：第一篇介绍attention的论文  
    [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
   
17. NAACL2016，Han Attention：attention用于文本分类  
    [Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)
    
19. Coling2018，SGM： 第一篇使用序列生成做多标签文本分类  
    [SGM: Sequence Generation Model for Multi-label Classification](https://arxiv.org/abs/1806.04822)
   
   
