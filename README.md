# NLP_paper2read
此仓库用于记录本人学习NLP的论文阅读顺序，如果能帮到一些小伙伴的话，It's my pleasure.

## survey
- Advances in natural language processing：对整个自然语言处理领域发展的综述
  
- Pre-trained Models for Natural Language Processing: A Survey 预训练模型综述
  
- Multimodal Foundation Models: From Specialists to General-Purpose Assistants：多模态大模型  

## baseline paper
- 【2013 ICLR】Word2Vec：词向量抗鼎之作，分为两篇，第一篇介绍CBOW和Skip-gram模型，第二篇介绍层次化softmax和负采样方法。
   [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)  
   [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)
   
- 【2014 EMNLP】Glove：词向量领域  
   [GloVe: Global Vectors for Word Representation](https://scholar.google.com/scholar?q=GloVe:+Global+Vectors+for+Word+Representation&hl=zh-CN&as_sdt=0&as_vis=1&oi=scholart)
   
- 【2015 EMNLP】 CharEmbedding：字符嵌入  
   [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)
   
- 【2014 EMNLP】TextCNN：CNN 用于文本分类  
   [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)
   
- 【2015 NIPS】CharTextCNN：字符级别的文本分类  
   [Character-level Convolutional Networks for Text Classification](https://arxiv.org/abs/1509.01626)
   
- 【2017 EACL】FastText：细粒度的文本分类  
    [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759)  
    
- 【2014 NLPS】DeepNMT：使用LSTM解决机器翻译问题  
    [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
    
- 【2015 ICLR】BahdanauNMT：第一篇介绍attention的论文  
    [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
   
- 【2016 NAACL】Han Attention：attention用于文本分类  
    [Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)
    
- 【2018 Coling】SGM： 第一篇使用序列生成做多标签文本分类  
    [SGM: Sequence Generation Model for Multi-label Classification](https://arxiv.org/abs/1806.04822)


## Pre-train models

- 【2017 NIPS】transformer：预训练的基石之作，必读
  [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

- 【2017 1CLR】transformer-xl
  
























  
   
