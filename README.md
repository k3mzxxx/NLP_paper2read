# NLP_paper2read
此仓库用于记录本人学习NLP的论文阅读顺序，如果能帮到一些小伙伴的话，It's my pleasure.

## survey
- Advances in natural language processing：对整个自然语言处理领域发展的综述
  
- Pre-trained Models for Natural Language Processing: A Survey 预训练模型综述
  
- Multimodal Foundation Models: From Specialists to General-Purpose Assistants：多模态大模型  

## baseline paper
- 【2013 ICLR】Word2Vec：词向量抗鼎之作，分为两篇，第一篇介绍CBOW和Skip-gram模型，第二篇介绍层次化softmax和负采样方法。
   [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)  
   [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)
   
- 【2014 EMNLP】Glove：词向量领域  
   [GloVe: Global Vectors for Word Representation](https://scholar.google.com/scholar?q=GloVe:+Global+Vectors+for+Word+Representation&hl=zh-CN&as_sdt=0&as_vis=1&oi=scholart)
   
- 【2015 EMNLP】 CharEmbedding：字符嵌入  
   [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)
   
- 【2014 EMNLP】TextCNN：CNN 用于文本分类  
   [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)
   
- 【2015 NIPS】CharTextCNN：字符级别的文本分类  
   [Character-level Convolutional Networks for Text Classification](https://arxiv.org/abs/1509.01626)
   
- 【2017 EACL】FastText：细粒度的文本分类  
    [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759)  
    
- 【2014 NLPS】DeepNMT：使用LSTM解决机器翻译问题  
    [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
    
- 【2015 ICLR】BahdanauNMT：第一篇介绍attention的论文  
    [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
   
- 【2016 NAACL】Han Attention：attention用于文本分类  
    [Hierarchical Attention Networks for Document Classification](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)
    
- 【2018 Coling】SGM： 第一篇使用序列生成做多标签文本分类  
    [SGM: Sequence Generation Model for Multi-label Classification](https://arxiv.org/abs/1806.04822)


## Pre-train models

- 【2017 NIPS】transformer：预训练的基石之作，必读  
  [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

- 【2017 1CLR】transformer-xl
  [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)

- 【2018 NAACL】elmo：动态词向量  
  [Deep contextualized word representations](https://arxiv.org/abs/1802.05365)  

- gpt
- 【2018 ACL】bert：基于Transformer的双向深度语言模型
  [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)  
- 【2018 ACL】ulmfit：少量样本训练的预训练模型  
  [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146)  
- 【2018 ICLR】albert：轻量级bert的代表之作  
  [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)  
- 【2019 PMLR】mass：包含gpt和bert的预训练模型  
  [MASS: Masked Sequence to Sequence Pre-training for Language Generation](https://arxiv.org/abs/1905.02450)  
- 【2019 NeuriPS】xlnet：自回归预训练模型代表之作  
  [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)  
- 【2020 ICLR】electra：轻量级新生代预训练模型  
  [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)  
  
  
























  
   
